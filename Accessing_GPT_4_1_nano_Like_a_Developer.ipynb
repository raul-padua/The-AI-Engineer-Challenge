{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1395.92s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi==0.115.12 (from -r api/requirements.txt (line 1))\n",
            "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn==0.34.2 (from -r api/requirements.txt (line 2))\n",
            "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting openai==1.77.0 (from -r api/requirements.txt (line 3))\n",
            "  Using cached openai-1.77.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic==2.11.4 (from -r api/requirements.txt (line 4))\n",
            "  Using cached pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
            "Collecting python-multipart==0.0.18 (from -r api/requirements.txt (line 5))\n",
            "  Using cached python_multipart-0.0.18-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi==0.115.12->-r api/requirements.txt (line 1))\n",
            "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/raulsallesdepadua/Documents/Dive_into_DL_book/.conda/lib/python3.11/site-packages (from fastapi==0.115.12->-r api/requirements.txt (line 1)) (4.12.2)\n",
            "Collecting click>=7.0 (from uvicorn==0.34.2->-r api/requirements.txt (line 2))\n",
            "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: h11>=0.8 in /Users/raulsallesdepadua/Documents/Dive_into_DL_book/.conda/lib/python3.11/site-packages (from uvicorn==0.34.2->-r api/requirements.txt (line 2)) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/raulsallesdepadua/Documents/Dive_into_DL_book/.conda/lib/python3.11/site-packages (from openai==1.77.0->-r api/requirements.txt (line 3)) (4.7.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai==1.77.0->-r api/requirements.txt (line 3))\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/raulsallesdepadua/Documents/Dive_into_DL_book/.conda/lib/python3.11/site-packages (from openai==1.77.0->-r api/requirements.txt (line 3)) (0.28.1)\n",
            "Collecting jiter<1,>=0.4.0 (from openai==1.77.0->-r api/requirements.txt (line 3))\n",
            "  Downloading jiter-0.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /Users/raulsallesdepadua/Documents/Dive_into_DL_book/.conda/lib/python3.11/site-packages (from openai==1.77.0->-r api/requirements.txt (line 3)) (1.3.1)\n",
            "Collecting tqdm>4 (from openai==1.77.0->-r api/requirements.txt (line 3))\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic==2.11.4->-r api/requirements.txt (line 4))\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic==2.11.4->-r api/requirements.txt (line 4))\n",
            "  Downloading pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic==2.11.4->-r api/requirements.txt (line 4))\n",
            "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/raulsallesdepadua/Documents/Dive_into_DL_book/.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai==1.77.0->-r api/requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/raulsallesdepadua/Documents/Dive_into_DL_book/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.77.0->-r api/requirements.txt (line 3)) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/raulsallesdepadua/Documents/Dive_into_DL_book/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.77.0->-r api/requirements.txt (line 3)) (1.0.7)\n",
            "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "Using cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "Using cached openai-1.77.0-py3-none-any.whl (662 kB)\n",
            "Using cached pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
            "Using cached python_multipart-0.0.18-py3-none-any.whl (24 kB)\n",
            "Downloading pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading jiter-0.10.0-cp311-cp311-macosx_11_0_arm64.whl (321 kB)\n",
            "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: typing-inspection, tqdm, python-multipart, pydantic-core, jiter, distro, click, annotated-types, uvicorn, starlette, pydantic, openai, fastapi\n",
            "Successfully installed annotated-types-0.7.0 click-8.2.1 distro-1.9.0 fastapi-0.115.12 jiter-0.10.0 openai-1.77.0 pydantic-2.11.4 pydantic-core-2.33.2 python-multipart-0.0.18 starlette-0.46.2 tqdm-4.67.1 typing-inspection-0.4.1 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r api/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJouXnUgKv",
        "outputId": "c6c25850-395d-4cbf-9d26-bfe9253d1711"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BlnPwwflwUeqJcwfMv9FD89vA6EiF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate the development of AI-powered applications involving large language models (LLMs). While they share some similarities, they serve different purposes and have distinct features. Here's a breakdown of their main differences:\\n\\n### Purpose and Focus\\n\\n**LangChain:**\\n- **Primary Focus:** Building complex, multi-step, chain-based AI applications.\\n- **Use Cases:** Conversational agents, automation workflows, language model integrations, and chaining multiple operations together.\\n- **Approach:** Provides a flexible framework to connect prompting, memory, tools, and APIs, enabling developers to orchestrate sophisticated language model applications.\\n\\n**LlamaIndex:**\\n- **Primary Focus:** Constructing, querying, and managing large document or data indexes for efficient retrieval.\\n- **Use Cases:** Building question-answering systems over large unstructured data (e.g., documents, PDFs, knowledge bases).\\n- **Approach:** Offers data ingestion, indexing, and retrieval mechanisms optimized for integrating knowledge bases with LLMs, facilitating contextual question answering.\\n\\n### Core Capabilities\\n\\n| Aspect | LangChain | LlamaIndex (GPT Index) |\\n|---------|------------|------------------------|\\n| **Main Functionality** | Orchestrates prompts, chains, tools, and workflows | Manages document indexes for retrieval and querying |\\n| **Data Handling** | Connects with APIs, tools, databases, and external data sources | Ingests unstructured data, builds indices for efficient access |\\n| **Application Type** | End-to-end AI app development, conversational AI, automation | Knowledge retrieval, question-answering, document-based AI apps |\\n| **Extensibility** | Highly customizable with numerous integrations | Focused on indexing strategies, retrieval algorithms |\\n\\n### Integration and Ecosystem\\n\\n- **LangChain:** Supports numerous LLM providers (OpenAI, Anthropic, Cohere, etc.), tools (search engines, databases), and custom prompts; designed for building multi-step workflows.\\n- **LlamaIndex:** Integrates with LLMs similarly but emphasizes ingestion and retrieval modules; supports various index types (e.g., list, tree, Vector store).\\n\\n### Summary\\n\\n| Aspect | LangChain | LlamaIndex |\\n|---------|------------|------------|\\n| **Main Goal** | Building dynamic, multi-stage AI workflows | Creating efficient, searchable knowledge bases for Q&A and retrieval |\\n| **Strengths** | Orchestration, chaining, multi-tool workflows | Indexing large document collections, fast retrieval |\\n| **Usage Scenario** | Automated chatbots, decision pipelines | Document question answering, knowledge retrieval |\\n\\n---\\n\\n### In short:\\n- **Use LangChain** if you're aiming to orchestrate complex AI workflows, build chatbots, or integrate multiple tools in a chain.\\n- **Use LlamaIndex** if your goal is to ingest large volumes of unstructured data and create systems that efficiently retrieve relevant information to answer questions.\\n\\n**Note:** Both frameworks can be complementary—LlamaIndex can be used for data ingestion and retrieval within a LangChain-powered application for added flexibility.\\n\\n---\\n\\nIf you'd like more specific guidance on choosing or implementing them, feel free to ask!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750731076, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=644, prompt_tokens=19, total_tokens=663, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as Godel/GPT Index) are both popular frameworks designed to facilitate building applications that leverage large language models (LLMs), but they have different focuses and functionalities.\n",
              "\n",
              "**1. Purpose and Focus**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Focuses on building *conversational AI applications*, including chatbots, question-answering systems, and complex multi-step workflows.\n",
              "  - Provides a modular framework for chaining together various components like prompts, models, memory, and data sources.\n",
              "  - Emphasizes *building applications* with a wide array of language model integrations, along with tools for state management, prompt management, and reasoning.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**\n",
              "  - Focuses primarily on *building indices* over external data sources (documents, PDFs, web pages) to enable efficient retrieval-augmented generation (RAG).\n",
              "  - Simplifies creating structured, queryable indices that are integrated with LLMs for question-answering and knowledge retrieval tasks.\n",
              "  - Major use case is enabling LLMs to reason over and retrieve information from large external datasets with easy-to-use indexing mechanisms.\n",
              "\n",
              "**2. Core Functionality**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Provides *chains* (sequences of calls to language models and tools).\n",
              "  - Supports *memory* to maintain state across interactions.\n",
              "  - Includes integrations with various tools, APIs, and data sources.\n",
              "  - Features *prompt engineering* utilities and *agent frameworks* that can decide what actions to take based on user inputs.\n",
              "  - Facilitates *multi-modal* workflows and complex reasoning chains.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Offers *indexing structures* such as ListIndex, TreeIndex, GraphIndex, etc.\n",
              "  - Enables *fast retrieval* of relevant data snippets for prompting LLMs.\n",
              "  - Focuses on *building, managing,* and *querying* over large external datasets.\n",
              "  - Simplifies the process of augmenting LLMs with external knowledge for improved accuracy.\n",
              "\n",
              "**3. Use Cases**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Chatbots and virtual assistants.\n",
              "  - Complex decision-making and prompting workflows.\n",
              "  - Tool integration and multi-step reasoning.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Knowledge base creation over large document collections.\n",
              "  - Information retrieval and question-answering over external data.\n",
              "  - Building knowledge augmented LLM applications.\n",
              "\n",
              "**4. Ecosystem and Compatibility**\n",
              "\n",
              "- Both frameworks are compatible with major LLM providers like OpenAI, Cohere, Hugging Face, etc.\n",
              "- LangChain offers a broader ecosystem for chaining and managing workflows.\n",
              "- LlamaIndex is more specialized for document indexing and retrieval.\n",
              "\n",
              "---\n",
              "\n",
              "**Summary:**\n",
              "- **LangChain** is a versatile framework for building sophisticated NLP applications involving chaining, tools, and memory, suitable for conversational agents and multi-step workflows.\n",
              "- **LlamaIndex** specializes in creating indices over external data sources to enable efficient retrieval and question-answering, enhancing LLM applications with external knowledge.\n",
              "\n",
              "Depending on your project needs—whether building complex conversational systems or integrating and querying large datasets—you might choose one or both frameworks to complement each other."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Honestly, I couldn't care less about ice right now! I'm so hungry that the only thing I want is some real food—crushed or cubed ice isn't going to fill the empty pit in my stomach. Just give me something nutritious already!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is fantastic for its refreshing and quick-cooling qualities, perfect for drinks like cocktails and smoothies! Cubed ice, on the other hand, is great when you want your drink to stay cold longer without diluting it too fast, like with whiskey or soda. Both have their own charm—depends on the mood! Which do you prefer?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BlnT2nsFQMYcXVkNtHBCR1k1lzjyU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think crushed ice is fantastic for its refreshing and quick-cooling qualities, perfect for drinks like cocktails and smoothies! Cubed ice, on the other hand, is great when you want your drink to stay cold longer without diluting it too fast, like with whiskey or soda. Both have their own charm—depends on the mood! Which do you prefer?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750731268, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=72, prompt_tokens=30, total_tokens=102, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure! Here's a sentence using the words 'stimple' and 'falbean':\n",
              "\n",
              "\"Amidst the lush meadow, the stimple breeze carried the faint scent of falbean blossoms, creating a serene atmosphere.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure! Here's an example sentence using both words:\n",
              "\n",
              "\"She grabbed the stimple wrench and the falbean to quickly tighten the machinery's bolt.\"\n",
              "\n",
              "(Note: Since 'falbean' is a tool that spins or rotates, it’s used here as a rotating or fastening device alongside the 'stimple' wrench, which is good and high quality.)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions - but it can still benefit from a Chain of Thought Prompt to increase the reliability of the response!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze Billy's options step by step.\n",
              "\n",
              "**Current time in local time zone:** 1PM  \n",
              "**Deadline (home time before 7PM EDT):** 7PM EDT\n",
              "\n",
              "---\n",
              "\n",
              "### Step 1: Convert local time to EDT or consider relative timing\n",
              "\n",
              "Since Billy wants to arrive **before 7PM EDT**, we need to compare the travel durations considering the time difference. But the problem doesn't specify the local time zone or the city Billy is in. \n",
              "\n",
              "**Assumption:**  \n",
              "- **San Francisco** is in Pacific Time (PT), which is **3 hours behind EDT**.\n",
              "- Therefore, **1PM PT** is **4PM EDT** (since PT + 3 hours = EDT).\n",
              "\n",
              "**Arrival deadline in local time (PT):**  \n",
              "- 7PM EDT = 4PM PT\n",
              "\n",
              "Billy's current local time is **1PM PT**, which is **3 hours before the deadline** in PT.\n",
              "\n",
              "---\n",
              "\n",
              "### Step 2: Determine total travel times for each option\n",
              "\n",
              "**Option 1:**  \n",
              "- Fly (3 hours)  \n",
              "- Bus (2 hours)  \n",
              "- **Total travel time:** 3 + 2 = **5 hours**  \n",
              "- Departure now (1PM PT)  \n",
              "- Arrival time if starting now: 1PM + 5 hours = **6PM PT**  \n",
              "- In EDT: 6PM PT + 3 hours = **9PM EDT**  \n",
              "- **Result:** Arrives after the deadline (9PM EDT > 7PM EDT) — **not timely**\n",
              "\n",
              "**Option 2:**  \n",
              "- Teleporter (0 hours)  \n",
              "- Bus (1 hour)  \n",
              "- **Total travel time:** 0 + 1 = **1 hour**  \n",
              "- Departure now (1PM PT)  \n",
              "- Arrival time: 1PM + 1 hour = **2PM PT**  \n",
              "- In EDT: 2PM PT + 3 hours = **5PM EDT**  \n",
              "- **Result:** Arrives before the deadline (5PM EDT < 7PM EDT) — **on time**\n",
              "\n",
              "---\n",
              "\n",
              "### **Conclusion:**\n",
              "\n",
              "- **If Billy chooses the flying + bus option, he will arrive at 9PM EDT, which is too late.**  \n",
              "- **If he takes the teleporter + bus, he will arrive at 5PM EDT, well before the deadline.**\n",
              "\n",
              "---\n",
              "\n",
              "### **Final answer:**\n",
              "\n",
              "**Yes, it does matter which option Billy chooses.** To arrive before 7PM EDT, he should take the teleporter and then the bus."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use the same prompt with a small modification - but this time include \"Let's think step by step\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's carefully analyze the scenario step by step.\n",
              "\n",
              "**Step 1: Understand Billy's current time and deadline**\n",
              "\n",
              "- Current local time: 1PM\n",
              "- Deadline to get home: before 7PM EDT\n",
              "\n",
              "**Step 2: Identify the travel options and their durations**\n",
              "\n",
              "Option 1:\n",
              "- Fly: 3 hours\n",
              "- Bus: 2 hours\n",
              "- Total time for Option 1: 3 + 2 = 5 hours\n",
              "\n",
              "Option 2:\n",
              "- Teleporter: 0 hours\n",
              "- Bus: 1 hour\n",
              "- Total time for Option 2: 0 + 1 = 1 hour\n",
              "\n",
              "**Step 3: Determine the time needed from now to reach home for each option**\n",
              "\n",
              "- **Option 1:**\n",
              "  - Total travel time: 5 hours\n",
              "  - Starting at 1PM, arrival time: 1PM + 5 hours = 6PM\n",
              "\n",
              "- **Option 2:**\n",
              "  - Total travel time: 1 hour\n",
              "  - Starting at 1PM, arrival time: 1PM + 1 hour = 2PM\n",
              "\n",
              "**Step 4: Check if Billy can arrive before 7PM**\n",
              "\n",
              "- Option 1: Arrival at 6PM, which is before 7PM. **Yes, he can get home on time.**\n",
              "- Option 2: Arrival at 2PM, which is also before 7PM. **Yes, he can get home on time.**\n",
              "\n",
              "**Step 5: Conclusion**\n",
              "\n",
              "Since both options allow Billy to get home before the 7PM deadline, **it doesn't matter which option he chooses in terms of meeting the deadline**.\n",
              "\n",
              "**Final answer:**\n",
              "**No, it does not matter which travel option Billy selects, because both options allow him to arrive before 7PM EDT.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \"\\nLet's think step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/The-AI-Engineer-Challenge) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
